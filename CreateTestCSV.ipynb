{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46db9004-9bbd-4548-b675-7205c1de3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a92c02c-a723-415b-9e36-b3f26037474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_data_preprocessor(forest_data):\n",
    "    targets = forest_data['Cover_Type']\n",
    "    \n",
    "    #Drop ID and target columns\n",
    "    training_features = forest_data.drop(['Cover_Type','Id'], axis=1)\n",
    "    \n",
    "    #Hydrology distance euclidean\n",
    "    water_dist = np.asarray([training_features['Horizontal_Distance_To_Hydrology'],training_features['Vertical_Distance_To_Hydrology']])\n",
    "    water_euclidean_dist = np.sqrt(np.square(water_dist[0]) + np.square(water_dist[1]))\n",
    "\n",
    "    training_features['Distance_To_Hydrology'] = pd.Series(water_euclidean_dist)\n",
    "    training_features = training_features.drop(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology'], axis=1)\n",
    "\n",
    "    #Sine of Aspect\n",
    "    aspect = np.asarray(training_features['Aspect'])\n",
    "    aspect_sine = np.sin(aspect * np.pi / 180)\n",
    "\n",
    "    training_features['Sine_Of_Aspect'] = pd.Series(aspect_sine)\n",
    "    \n",
    "    training_features = training_features.drop(['Aspect'], axis=1)\n",
    "    \n",
    "    #Average Hillshade\n",
    "    avg_hillshade = np.asarray([training_features['Hillshade_9am'],training_features['Hillshade_Noon'],training_features['Hillshade_3pm']])\n",
    "    avg_hillshade = (avg_hillshade[0] + avg_hillshade[1] + avg_hillshade[2]) / 3\n",
    "    \n",
    "    training_features['Average_Hillshade'] = pd.Series(avg_hillshade)\n",
    "    \n",
    "    #Drop remaining unwanted features\n",
    "    #training_features = training_features.drop(['Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'], axis=1)\n",
    "    training_features = training_features.drop(['Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm'], axis=1)\n",
    "\n",
    "\n",
    "    soil_groups = [ \n",
    "                    [1,2,3,4,5,6,7,8,9],\n",
    "                    [10,11,12,13,14,16,17],\n",
    "                    [18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\n",
    "                    [34,35,36,37,38,39,40]\n",
    "                ]\n",
    "\n",
    "    for i in range(len(soil_groups)):\n",
    "        soil_group = 'Soil_Group' + str(i+1)\n",
    "        training_features[soil_group] = pd.Series(np.zeros_like(np.asarray(training_features['Soil_Type1'])))\n",
    "        for j in soil_groups[i]:\n",
    "            soil_type = 'Soil_Type' + str(j)\n",
    "            training_features[soil_group] += training_features[soil_type]\n",
    "\n",
    "    soil_types = []\n",
    "    for i in range(1,41):\n",
    "        soil_type = 'Soil_Type' + str(i)\n",
    "        soil_types.append(soil_type)\n",
    "\n",
    "    training_features = training_features.drop(soil_types, axis=1)\n",
    "\n",
    "    \n",
    "    return training_features, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b882d4d-2a8d-47e0-87fb-877d5519c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_preprocessor(forest_data):\n",
    "    #Drop ID column\n",
    "    features = forest_data.drop(['Id'], axis=1)\n",
    "    \n",
    "    #Hydrology distance euclidean\n",
    "    water_dist = np.asarray([features['Horizontal_Distance_To_Hydrology'],features['Vertical_Distance_To_Hydrology']])\n",
    "    water_euclidean_dist = np.sqrt(np.square(water_dist[0]) + np.square(water_dist[1]))\n",
    "\n",
    "    features['Distance_To_Hydrology'] = pd.Series(water_euclidean_dist)\n",
    "    features = features.drop(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology'], axis=1)\n",
    "\n",
    "    #Sine of Aspect\n",
    "    aspect = np.asarray(features['Aspect'])\n",
    "    aspect_sine = np.sin(aspect * np.pi / 180)\n",
    "\n",
    "    features['Sine_Of_Aspect'] = pd.Series(aspect_sine)\n",
    "    \n",
    "    features = features.drop(['Aspect'], axis=1)\n",
    "    \n",
    "    #Average Hillshade\n",
    "    avg_hillshade = np.asarray([features['Hillshade_9am'],features['Hillshade_Noon'],features['Hillshade_3pm']])\n",
    "    avg_hillshade = (avg_hillshade[0] + avg_hillshade[1] + avg_hillshade[2]) / 3\n",
    "    \n",
    "    features['Average_Hillshade'] = pd.Series(avg_hillshade)\n",
    "    \n",
    "    #Drop remaining unwanted features\n",
    "    #features = features.drop(['Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'], axis=1)\n",
    "    features = features.drop(['Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm'], axis=1)\n",
    "\n",
    "\n",
    "    soil_groups = [ \n",
    "                    [1,2,3,4,5,6,7,8,9],\n",
    "                    [10,11,12,13,14,16,17],\n",
    "                    [18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\n",
    "                    [34,35,36,37,38,39,40]\n",
    "                ]\n",
    "\n",
    "    for i in range(len(soil_groups)):\n",
    "        soil_group = 'Soil_Group' + str(i+1)\n",
    "        features[soil_group] = pd.Series(np.zeros_like(np.asarray(features['Soil_Type1'])))\n",
    "        for j in soil_groups[i]:\n",
    "            soil_type = 'Soil_Type' + str(j)\n",
    "            features[soil_group] += features[soil_type]\n",
    "\n",
    "    soil_types = []\n",
    "    for i in range(1,41):\n",
    "        soil_type = 'Soil_Type' + str(i)\n",
    "        soil_types.append(soil_type)\n",
    "\n",
    "    features = features.drop(soil_types, axis=1)\n",
    "\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eadbfc1-a3c3-46b6-8b55-2718d57aea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_test(c_val, max_it, random_seed):\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(train_df)\n",
    "\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    test_features = test_data_preprocessor(test_df)\n",
    "    \n",
    "    #Fix Dummy Variable Trap\n",
    "    training_features = training_features.drop(['Wilderness_Area4','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    forest_data_scaled = min_max_scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a logistic regression model for our data using Sklearn\n",
    "    logistic_regression_model = LogisticRegression(max_iter=max_it, random_state=random_seed, solver='sag', C=c_val)\n",
    "    logistic_regression_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    #Fix Dummy Variable Trap\n",
    "    test_features = test_features.drop(['Wilderness_Area4','Soil_Group4'], axis=1)\n",
    "\n",
    "    test_features_scaled = min_max_scaler.transform(np.asarray(test_features))\n",
    "\n",
    "    test_cover_type = logistic_regression_model.predict(test_features_scaled)\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output['Id'] = test_df['Id']\n",
    "    output['Cover_Type'] = test_cover_type\n",
    "    \n",
    "    output.to_csv(path_or_buf='logistic_regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88c2214-6b0e-4479-9a6d-910cd25eb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_test(num_neigh,categorical_scale_factor):\n",
    "    \n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    test_features = test_data_preprocessor(test_df)\n",
    "    \n",
    "    temp_df = training_features.drop(['Elevation', 'Average_Hillshade', 'Sine_Of_Aspect', 'Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways', 'Slope'], axis=1)\n",
    "    training_features = training_features.drop(['Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], axis=1)\n",
    "    \n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        soil_type = 'Soil_Group' + str(i)\n",
    "        temp_df[soil_type] = temp_df[soil_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        temp_df[wilderness_type] = temp_df[wilderness_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    temp_df = np.asarray(temp_df)\n",
    "\n",
    "    forest_data_scaled = np.concatenate((forest_data_scaled, temp_df), axis=1)\n",
    "\n",
    "    # Create a kNN model for our data using Sklearn\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=num_neigh, weights='distance', algorithm='brute')\n",
    "    knn_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    #Prepare test data\n",
    "    example_temp1 = test_features.drop(['Elevation', 'Average_Hillshade', 'Sine_Of_Aspect', 'Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways', 'Slope'], axis=1)\n",
    "    example_temp2 = test_features.drop(['Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], axis=1)\n",
    "\n",
    "    example_temp2 = np.asarray(example_temp2)\n",
    "    example_scaled = scaler.transform(example_temp2)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        soil_type = 'Soil_Group' + str(i)\n",
    "        example_temp1[soil_type] = example_temp1[soil_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        example_temp1[wilderness_type] = example_temp1[wilderness_type].multiply(categorical_scale_factor)\n",
    "    \n",
    "    example_temp1 = np.asarray(example_temp1)\n",
    "\n",
    "    example_processed = np.concatenate((example_scaled, example_temp1), axis=1)\n",
    "\n",
    "    test_cover_type = knn_model.predict(example_processed)\n",
    "    \n",
    "    output = pd.DataFrame()\n",
    "    output['Id'] = test_df['Id']\n",
    "    output['Cover_Type'] = test_cover_type\n",
    "    \n",
    "    output.to_csv(path_or_buf='kNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdccee35-3306-47fd-973b-30c4bcdf08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_tree_test(criterion, max_depth, random_seed):\n",
    "\n",
    "    the_c = 'gini'\n",
    "    if criterion == 1:\n",
    "        the_c = 'entropy'\n",
    "    \n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    example = test_data_preprocessor(test_df)\n",
    "\n",
    "    # Integer labeling\n",
    "    training_features['Wilderness_Area'] = np.asarray(training_features['Wilderness_Area1']) + 2*np.asarray(training_features['Wilderness_Area2']) + 3*np.asarray(training_features['Wilderness_Area3']) + 4*np.asarray(training_features['Wilderness_Area4'])\n",
    "    training_features['Soil_Group'] = np.asarray(training_features['Soil_Group1']) + 2*np.asarray(training_features['Soil_Group2']) + 3*np.asarray(training_features['Soil_Group3']) + 4*np.asarray(training_features['Soil_Group4'])\n",
    "\n",
    "    training_features = training_features.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a decision tree classifier model for our data using Sklearn\n",
    "    decision_tree_model = DecisionTreeClassifier(criterion=the_c, max_depth=max_depth, random_state=random_seed)\n",
    "    decision_tree_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    example['Wilderness_Area'] = np.asarray(example['Wilderness_Area1']) + 2*np.asarray(example['Wilderness_Area2']) + 3*np.asarray(example['Wilderness_Area3']) + 4*np.asarray(example['Wilderness_Area4'])\n",
    "    example['Soil_Group'] = np.asarray(example['Soil_Group1']) + 2*np.asarray(example['Soil_Group2']) + 3*np.asarray(example['Soil_Group3']) + 4*np.asarray(example['Soil_Group4'])\n",
    "\n",
    "    example = example.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "\n",
    "    example_scaled = scaler.transform(np.asarray(example))\n",
    "    \n",
    "    test_cover_type = decision_tree_model.predict(example_scaled)\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output['Id'] = test_df['Id']\n",
    "    output['Cover_Type'] = test_cover_type\n",
    "    \n",
    "    output.to_csv(path_or_buf='decision_tree.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4add6a-8d47-46c7-b306-187cb18ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_test(num_estimators, criterion, max_depth, random_seed):\n",
    "    the_c = 'gini'\n",
    "    if criterion == 1:\n",
    "        the_c = 'entropy'\n",
    "\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    example = test_data_preprocessor(test_df)\n",
    "\n",
    "    training_features['Wilderness_Area'] = np.asarray(training_features['Wilderness_Area1']) + 2*np.asarray(training_features['Wilderness_Area2']) + 3*np.asarray(training_features['Wilderness_Area3']) + 4*np.asarray(training_features['Wilderness_Area4'])\n",
    "    training_features['Soil_Group'] = np.asarray(training_features['Soil_Group1']) + 2*np.asarray(training_features['Soil_Group2']) + 3*np.asarray(training_features['Soil_Group3']) + 4*np.asarray(training_features['Soil_Group4'])\n",
    "\n",
    "    training_features = training_features.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "\n",
    " \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a random forest classifier model for our data using Sklearn\n",
    "    rf_model = RandomForestClassifier(num_estimators, criterion=the_c, max_depth=max_depth, random_state=random_seed)\n",
    "    rf_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    example['Wilderness_Area'] = np.asarray(example['Wilderness_Area1']) + 2*np.asarray(example['Wilderness_Area2']) + 3*np.asarray(example['Wilderness_Area3']) + 4*np.asarray(example['Wilderness_Area4'])\n",
    "    example['Soil_Group'] = np.asarray(example['Soil_Group1']) + 2*np.asarray(example['Soil_Group2']) + 3*np.asarray(example['Soil_Group3']) + 4*np.asarray(example['Soil_Group4'])\n",
    "\n",
    "    example = example.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "\n",
    "    example_scaled = scaler.transform(np.asarray(example))\n",
    "    \n",
    "    test_cover_type = rf_model.predict(example_scaled)\n",
    "\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output['Id'] = test_df['Id']\n",
    "    output['Cover_Type'] = test_cover_type\n",
    "    \n",
    "    output.to_csv(path_or_buf='random_forest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9fe1c53-2f34-42c1-af5c-40639d56abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_test(64,600,13124)\n",
    "knn_test(1,5)\n",
    "d_tree_test(1,13,13124)\n",
    "rf_test(150,1,19,13124)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
