{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8d5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2adaf1-2cee-4ec0-adf3-a9baf24def35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_predict(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,c_val, max_it, random_seed):\n",
    "    example = predictor_preprocessor(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil)\n",
    "\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "    \n",
    "    #Fix Dummy Variable Trap\n",
    "    training_features = training_features.drop(['Wilderness_Area4','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    forest_data_scaled = min_max_scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a logistic regression model for our data using Sklearn\n",
    "    logistic_regression_model = LogisticRegression(max_iter=max_it, random_state=random_seed, solver='sag', C=c_val)\n",
    "    logistic_regression_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    #Fix Dummy Variable Trap\n",
    "    example = example.drop(['Wilderness_Area4','Soil_Group4'], axis=1)\n",
    "\n",
    "    example_scaled = min_max_scaler.transform(np.asarray(example))\n",
    "\n",
    "    forest_cov = logistic_regression_model.predict(example_scaled)\n",
    "    \n",
    "\n",
    "    return {\n",
    "            forest_cover: forest_cov[0],\n",
    "            forest_cover_name: get_forest_cover_name(forest_cov[0])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e06af60-4366-431c-8778-6d29beca43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,num_neigh,categorical_scale_factor):\n",
    "    example = predictor_preprocessor(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil)\n",
    "\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "    \n",
    "    temp_df = training_features.drop(['Elevation', 'Average_Hillshade', 'Sine_Of_Aspect', 'Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways', 'Slope'], axis=1)\n",
    "    training_features = training_features.drop(['Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], axis=1)\n",
    "    \n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        soil_type = 'Soil_Group' + str(i)\n",
    "        temp_df[soil_type] = temp_df[soil_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        temp_df[wilderness_type] = temp_df[wilderness_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    temp_df = np.asarray(temp_df)\n",
    "\n",
    "    forest_data_scaled = np.concatenate((forest_data_scaled, temp_df), axis=1)\n",
    "\n",
    "    # Create a kNN model for our data using Sklearn\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=num_neigh, weights='distance', algorithm='brute')\n",
    "    knn_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    #Prepare example\n",
    "    example_temp1 = example.drop(['Elevation', 'Average_Hillshade', 'Sine_Of_Aspect', 'Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways', 'Slope'], axis=1)\n",
    "    example_temp2 = example.drop(['Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], axis=1)\n",
    "\n",
    "    example_temp2 = np.asarray(example_temp2)\n",
    "    example_scaled = scaler.transform(example_temp2)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        soil_type = 'Soil_Group' + str(i)\n",
    "        example_temp1[soil_type] = example_temp1[soil_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        example_temp1[wilderness_type] = example_temp1[wilderness_type].multiply(categorical_scale_factor)\n",
    "    \n",
    "    example_temp1 = np.asarray(example_temp1)\n",
    "\n",
    "    example_processed = np.concatenate((example_scaled, example_temp1), axis=1)\n",
    "\n",
    "    forest_cov = knn_model.predict(example_processed)\n",
    "    \n",
    "\n",
    "    return {\n",
    "            forest_cover: forest_cov[0],\n",
    "            forest_cover_name: get_forest_cover_name(forest_cov[0])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdc9368-9ace-48b8-8818-d31067038e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_tree_predict(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,criterion, max_depth, random_seed):\n",
    "    example = predictor_preprocessor(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil)\n",
    "    \n",
    "    the_c = 'gini'\n",
    "    if criterion == 1:\n",
    "        the_c = 'entropy'\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    # Integer labeling\n",
    "    training_features['Wilderness_Area'] = np.asarray(training_features['Wilderness_Area1']) + 2*np.asarray(training_features['Wilderness_Area2']) + 3*np.asarray(training_features['Wilderness_Area3']) + 4*np.asarray(training_features['Wilderness_Area4'])\n",
    "    training_features['Soil_Group'] = np.asarray(training_features['Soil_Group1']) + 2*np.asarray(training_features['Soil_Group2']) + 3*np.asarray(training_features['Soil_Group3']) + 4*np.asarray(training_features['Soil_Group4'])\n",
    "\n",
    "    training_features = training_features.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a decision tree classifier model for our data using Sklearn\n",
    "    decision_tree_model = DecisionTreeClassifier(criterion=the_c, max_depth=max_depth, random_state=random_seed)\n",
    "    decision_tree_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    example['Wilderness_Area'] = np.asarray(example['Wilderness_Area1']) + 2*np.asarray(example['Wilderness_Area2']) + 3*np.asarray(example['Wilderness_Area3']) + 4*np.asarray(example['Wilderness_Area4'])\n",
    "    example['Soil_Group'] = np.asarray(example['Soil_Group1']) + 2*np.asarray(example['Soil_Group2']) + 3*np.asarray(example['Soil_Group3']) + 4*np.asarray(example['Soil_Group4'])\n",
    "\n",
    "    example = example.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "\n",
    "    example_scaled = scaler.transform(np.asarray(example))\n",
    "    \n",
    "    forest_cov = decision_tree_model.predict(example_scaled)\n",
    "\n",
    "    \n",
    "    return {\n",
    "            forest_cover: forest_cov[0],\n",
    "            forest_cover_name: get_forest_cover_name(forest_cov[0])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7895be49-0c75-4c5f-827d-e4a57dd38403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_predict(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,num_estimators, criterion, max_depth, random_seed):\n",
    "    example = predictor_preprocessor(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil)\n",
    "\n",
    "    the_c = 'gini'\n",
    "    if criterion == 1:\n",
    "        the_c = 'entropy'\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    training_features['Wilderness_Area'] = np.asarray(training_features['Wilderness_Area1']) + 2*np.asarray(training_features['Wilderness_Area2']) + 3*np.asarray(training_features['Wilderness_Area3']) + 4*np.asarray(training_features['Wilderness_Area4'])\n",
    "    training_features['Soil_Group'] = np.asarray(training_features['Soil_Group1']) + 2*np.asarray(training_features['Soil_Group2']) + 3*np.asarray(training_features['Soil_Group3']) + 4*np.asarray(training_features['Soil_Group4'])\n",
    "\n",
    "    training_features = training_features.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a random forest classifier model for our data using Sklearn\n",
    "    rf_model = RandomForestClassifier(num_estimators, criterion=the_c, max_depth=max_depth, random_state=random_seed)\n",
    "    rf_model.fit(forest_data_scaled, forest_targets)\n",
    "\n",
    "    example['Wilderness_Area'] = np.asarray(example['Wilderness_Area1']) + 2*np.asarray(example['Wilderness_Area2']) + 3*np.asarray(example['Wilderness_Area3']) + 4*np.asarray(example['Wilderness_Area4'])\n",
    "    example['Soil_Group'] = np.asarray(example['Soil_Group1']) + 2*np.asarray(example['Soil_Group2']) + 3*np.asarray(example['Soil_Group3']) + 4*np.asarray(example['Soil_Group4'])\n",
    "\n",
    "    example = example.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "\n",
    "    example_scaled = scaler.transform(np.asarray(example))\n",
    "    \n",
    "    forest_cov = rf_model.predict(example_scaled)\n",
    "\n",
    "\n",
    "    \n",
    "    return {\n",
    "            forest_cover: forest_cov[0],\n",
    "            forest_cover_name: get_forest_cover_name(forest_cov[0])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b14035f-d66f-4271-ab58-d281f449f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest_cover_name(cover_type):\n",
    "    if (cover_type == 1):\n",
    "        return 'Spruce/Fir'\n",
    "    elif (cover_type == 2):\n",
    "        return 'Lodgepole Pine'\n",
    "    elif (cover_type == 3):\n",
    "        return 'Ponderosa Pine'\n",
    "    elif (cover_type == 4):\n",
    "        return 'Cottonwood/Willow'\n",
    "    elif (cover_type == 5):\n",
    "        return 'Aspen'\n",
    "    elif (cover_type == 6):\n",
    "        return 'Douglas-fir'\n",
    "    else:\n",
    "        return 'Krummholz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b57c7f-5f51-420e-be11-3863396ba0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_preprocessor(elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil):\n",
    "    example = pd.DataFrame()\n",
    "    #Placeholder since Id column gets dropped\n",
    "    example['Id'] = [0]\n",
    "    example['Elevation'] = [elevation]\n",
    "    example['Aspect'] = [aspect]\n",
    "    example['Slope'] = [slope]\n",
    "    example['Horizontal_Distance_To_Hydrology'] = [h_dist_hydro]\n",
    "    example['Vertical_Distance_To_Hydrology'] = [v_dist_hydro]\n",
    "    example['Horizontal_Distance_To_Roadways'] = [h_dist_road]\n",
    "    example['Hillshade_9am'] = [shade_9]\n",
    "    example['Hillshade_Noon'] = [shade_12]\n",
    "    example['Hillshade_3pm'] = [shade_3]\n",
    "    example['Horizontal_Distance_To_Fire_Points'] = [h_dist_fire]\n",
    "\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        if (wilderness == i):\n",
    "            example[wilderness_type] = [1]\n",
    "        else:\n",
    "            example[wilderness_type] = [0]\n",
    "\n",
    "    for i in range(1,41):\n",
    "        soil_type = 'Soil_Type' + str(i)\n",
    "        if (soil == i):\n",
    "            example[soil_type] = [1]\n",
    "        else:\n",
    "            example[soil_type] = [0]\n",
    "    \n",
    "    #Drop ID columns\n",
    "    example = example.drop(['Id'], axis=1)\n",
    "    \n",
    "    #Hydrology distance euclidean\n",
    "    water_dist = np.asarray([example['Horizontal_Distance_To_Hydrology'],example['Vertical_Distance_To_Hydrology']])\n",
    "    water_euclidean_dist = np.sqrt(np.square(water_dist[0]) + np.square(water_dist[1]))\n",
    "\n",
    "    example['Distance_To_Hydrology'] = pd.Series(water_euclidean_dist)\n",
    "    example = example.drop(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology'], axis=1)\n",
    "\n",
    "    #Sine of Aspect\n",
    "    aspect = np.asarray(example['Aspect'])\n",
    "    aspect_sine = np.sin(aspect * np.pi / 180)\n",
    "\n",
    "    example['Sine_Of_Aspect'] = pd.Series(aspect_sine)\n",
    "    \n",
    "    example = example.drop(['Aspect'], axis=1)\n",
    "    \n",
    "    #Average Hillshade\n",
    "    avg_hillshade = np.asarray([example['Hillshade_9am'],example['Hillshade_Noon'],example['Hillshade_3pm']])\n",
    "    avg_hillshade = (avg_hillshade[0] + avg_hillshade[1] + avg_hillshade[2]) / 3\n",
    "    \n",
    "    example['Average_Hillshade'] = pd.Series(avg_hillshade)\n",
    "    \n",
    "    #Drop remaining unwanted features\n",
    "    #training_features = training_features.drop(['Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'], axis=1)\n",
    "    example = example.drop(['Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm'], axis=1)\n",
    "\n",
    "\n",
    "    soil_groups = [ \n",
    "                    [1,2,3,4,5,6,7,8,9],\n",
    "                    [10,11,12,13,14,16,17],\n",
    "                    [18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\n",
    "                    [34,35,36,37,38,39,40]\n",
    "                ]\n",
    "\n",
    "    for i in range(len(soil_groups)):\n",
    "        soil_group = 'Soil_Group' + str(i+1)\n",
    "        example[soil_group] = pd.Series(np.zeros_like(np.asarray(example['Soil_Type1'])))\n",
    "        for j in soil_groups[i]:\n",
    "            soil_type = 'Soil_Type' + str(j)\n",
    "            example[soil_group] += example[soil_type]\n",
    "\n",
    "    soil_types = []\n",
    "    for i in range(1,41):\n",
    "        soil_type = 'Soil_Type' + str(i)\n",
    "        soil_types.append(soil_type)\n",
    "\n",
    "    example = example.drop(soil_types, axis=1)\n",
    "    \n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088fb86f-a008-4ef9-b706-5cd00bbe372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_data_preprocessor(forest_data):\n",
    "    targets = forest_data['Cover_Type']\n",
    "    \n",
    "    #Drop ID and target columns\n",
    "    training_features = forest_data.drop(['Cover_Type','Id'], axis=1)\n",
    "    \n",
    "    #Hydrology distance euclidean\n",
    "    water_dist = np.asarray([training_features['Horizontal_Distance_To_Hydrology'],training_features['Vertical_Distance_To_Hydrology']])\n",
    "    water_euclidean_dist = np.sqrt(np.square(water_dist[0]) + np.square(water_dist[1]))\n",
    "\n",
    "    training_features['Distance_To_Hydrology'] = pd.Series(water_euclidean_dist)\n",
    "    training_features = training_features.drop(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology'], axis=1)\n",
    "\n",
    "    #Sine of Aspect\n",
    "    aspect = np.asarray(training_features['Aspect'])\n",
    "    aspect_sine = np.sin(aspect * np.pi / 180)\n",
    "\n",
    "    training_features['Sine_Of_Aspect'] = pd.Series(aspect_sine)\n",
    "    \n",
    "    training_features = training_features.drop(['Aspect'], axis=1)\n",
    "    \n",
    "    #Average Hillshade\n",
    "    avg_hillshade = np.asarray([training_features['Hillshade_9am'],training_features['Hillshade_Noon'],training_features['Hillshade_3pm']])\n",
    "    avg_hillshade = (avg_hillshade[0] + avg_hillshade[1] + avg_hillshade[2]) / 3\n",
    "    \n",
    "    training_features['Average_Hillshade'] = pd.Series(avg_hillshade)\n",
    "    \n",
    "    #Drop remaining unwanted features\n",
    "    #training_features = training_features.drop(['Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'], axis=1)\n",
    "    training_features = training_features.drop(['Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm'], axis=1)\n",
    "\n",
    "\n",
    "    soil_groups = [ \n",
    "                    [1,2,3,4,5,6,7,8,9],\n",
    "                    [10,11,12,13,14,16,17],\n",
    "                    [18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\n",
    "                    [34,35,36,37,38,39,40]\n",
    "                ]\n",
    "\n",
    "    for i in range(len(soil_groups)):\n",
    "        soil_group = 'Soil_Group' + str(i+1)\n",
    "        training_features[soil_group] = pd.Series(np.zeros_like(np.asarray(training_features['Soil_Type1'])))\n",
    "        for j in soil_groups[i]:\n",
    "            soil_type = 'Soil_Type' + str(j)\n",
    "            training_features[soil_group] += training_features[soil_type]\n",
    "\n",
    "    soil_types = []\n",
    "    for i in range(1,41):\n",
    "        soil_type = 'Soil_Type' + str(i)\n",
    "        soil_types.append(soil_type)\n",
    "\n",
    "    training_features = training_features.drop(soil_types, axis=1)\n",
    "\n",
    "    \n",
    "    return training_features, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a8999b-9890-4ded-a81a-b4bcde17d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_visualization():\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "\n",
    "    fig1 = categorical_visualization(df)\n",
    "\n",
    "    fig2 = numerical_visualization(training_features, targets)\n",
    "\n",
    "    return {\n",
    "            bar_charts: gr.Plot(fig1, visible=True),\n",
    "            histograms: gr.Plot(fig2, visible=True)\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9a8717-2366-4182-918d-f0c0e7461cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_visualization(df):\n",
    "    my_figure = plt.figure(1, figsize=(10, 10))\n",
    "    \n",
    "    #All examples have a soil type\n",
    "    soil_counts = np.zeros((40,7))\n",
    "    for i in range(1,41):\n",
    "        soil_type = 'Soil_Type' + str(i)\n",
    "        for j in range(1,8):\n",
    "            soil_counts[i-1][j-1] = df[soil_type][df[soil_type] == 1][df['Cover_Type'] == j].count()\n",
    "    \n",
    "    \n",
    "    #All examples have a wilderness area\n",
    "    wild_counts = np.zeros((4,7))\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        for j in range(1,8):\n",
    "            wild_counts[i-1][j-1] = df[wilderness_type][df[wilderness_type] == 1][df['Cover_Type'] == j].count()\n",
    "    \n",
    "    bottom = np.zeros(4)\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    for i in range(7):\n",
    "        plt.bar([1,2,3,4],wild_counts[:,i], bottom = bottom)\n",
    "        bottom += wild_counts[:,i]\n",
    "    \n",
    "    plt.xticks([1,2,3,4])\n",
    "    plt.title('Wilderness Area Stacked Bar Chart')\n",
    "    plt.legend(['1','2','3','4','5','6','7'], bbox_to_anchor=(1.025,1))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Wilderness Area')\n",
    "    \n",
    "    \n",
    "    bottom = np.zeros(40)\n",
    "    plt.subplot(3, 1, 2)\n",
    "    for i in range(7):\n",
    "        plt.bar(range(1,41) , soil_counts[:,i], bottom = bottom)\n",
    "        bottom += soil_counts[:,i]\n",
    "    \n",
    "    plt.xticks(range(1,41,2))\n",
    "    plt.title('Soil Type Stacked Bar Chart')\n",
    "    plt.legend(['1','2','3','4','5','6','7'], bbox_to_anchor=(1.025,1))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Soil Type')\n",
    "\n",
    "\n",
    "    soil_groups = [ \n",
    "                    [1,2,3,4,5,6,7,8,9],\n",
    "                    [10,11,12,13,14,16,17],\n",
    "                    [18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],\n",
    "                    [34,35,36,37,38,39,40]\n",
    "                ]\n",
    "    \n",
    "    soil_group_counts = np.zeros((4,7))\n",
    "\n",
    "    for i,soil_group in enumerate(soil_groups):\n",
    "        for j in soil_group:\n",
    "            for k in range(7):\n",
    "                soil_group_counts[i][k] += soil_counts[j-1][k]\n",
    "\n",
    "    bottom = np.zeros(4)\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    for i in range(7):\n",
    "        plt.bar([1,2,3,4],soil_group_counts[:,i], bottom = bottom)\n",
    "        bottom += soil_group_counts[:,i]\n",
    "    \n",
    "    plt.xticks([1,2,3,4])\n",
    "    plt.title('Soil Group Stacked Bar Chart')\n",
    "    plt.legend(['1','2','3','4','5','6','7'], bbox_to_anchor=(1.025,1))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Soil Group')\n",
    "            \n",
    "\n",
    "    \n",
    "    return my_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd204896-1681-45fe-a1a7-e38f3ccf07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_visualization(training_features, targets):\n",
    "    my_figure = plt.figure(2, figsize=(10, 10))\n",
    "    indices = ['Elevation', 'Sine_Of_Aspect', 'Slope', 'Distance_To_Hydrology',  'Horizontal_Distance_To_Roadways', 'Average_Hillshade', 'Horizontal_Distance_To_Fire_Points']\n",
    "    units = ['Meters','Sine of Aspect in Degrees Azimuth', 'Degrees', 'Distance to Nearest Surface Water Features', 'Distance to Nearest Roadway', 'Average Hillshade Index During Summer Solstice','Distance to Nearest Wildfire Ignition Points']\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.5, hspace=1)\n",
    "    for h,i in enumerate(indices):\n",
    "        plt.subplot(4,2, h+1)\n",
    "        plt.hist([training_features[i][targets == 1],training_features[i][targets == 2],training_features[i][targets == 3],training_features[i][targets == 4],training_features[i][targets == 5],training_features[i][targets == 6],training_features[i][targets == 7]], histtype='step',label=['1','2','3','4','5','6','7'])\n",
    "        plt.title(i.replace('_', ' ') + \" Histogram\")\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xlabel(units[h])\n",
    "        plt.legend(bbox_to_anchor=(1.025,1))\n",
    "        \n",
    "    return my_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "874b3a8c-0c9a-4af3-8e05-847ea3fc41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_logreg(c_val, max_it, random_seed, num_folds):\n",
    "\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "    \n",
    "    #Fix Dummy Variable Trap\n",
    "    training_features = training_features.drop(['Wilderness_Area4','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    forest_data_scaled = min_max_scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a logistic regression model for our data using Sklearn\n",
    "    logistic_regression_model = LogisticRegression(max_iter=max_it, random_state=random_seed, solver='sag', C=c_val)\n",
    "    \n",
    "    cross_validator_results = cross_validate(logistic_regression_model, forest_data_scaled, forest_targets, scoring=['accuracy','f1_weighted'], cv=num_folds)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "            output_col: gr.Column(visible=True),\n",
    "            avg_accuracy: np.mean(cross_validator_results['test_accuracy']),\n",
    "            avg_stdev: np.std(cross_validator_results['test_accuracy'], ddof=1),\n",
    "            avg_f1: np.mean(cross_validator_results['test_f1_weighted']),\n",
    "            f1_stdev: np.std(cross_validator_results['test_f1_weighted'], ddof=1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "812d1fa1-2842-400c-9ff2-861c56076298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_knn(num_neigh, categorical_scale_factor, num_folds):\n",
    "\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "    \n",
    "    temp_df = training_features.drop(['Elevation', 'Average_Hillshade', 'Sine_Of_Aspect', 'Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways', 'Slope'], axis=1)\n",
    "    training_features = training_features.drop(['Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], axis=1)\n",
    "    \n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        soil_type = 'Soil_Group' + str(i)\n",
    "        temp_df[soil_type] = temp_df[soil_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    for i in range(1,5):\n",
    "        wilderness_type = 'Wilderness_Area' + str(i)\n",
    "        temp_df[wilderness_type] = temp_df[wilderness_type].multiply(categorical_scale_factor)\n",
    "\n",
    "    temp_df = np.asarray(temp_df)\n",
    "\n",
    "    forest_data_scaled = np.concatenate((forest_data_scaled, temp_df), axis=1)\n",
    "    \n",
    "    #print(forest_data_scaled[:,0:10])\n",
    "\n",
    "    # Create a kNN model for our data using Sklearn\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=num_neigh, weights='distance', algorithm='brute')\n",
    "\n",
    "\n",
    "    cross_validator_results = cross_validate(knn_model, forest_data_scaled, forest_targets, scoring=['accuracy','f1_weighted'], cv=num_folds)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "            output_col: gr.Column(visible=True),\n",
    "            avg_accuracy: np.mean(cross_validator_results['test_accuracy']),\n",
    "            avg_stdev: np.std(cross_validator_results['test_accuracy'], ddof=1),\n",
    "            avg_f1: np.mean(cross_validator_results['test_f1_weighted']),\n",
    "            f1_stdev: np.std(cross_validator_results['test_f1_weighted'], ddof=1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2312f1a-45bd-4450-b612-09cf2ad06051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dtree(criterion, max_depth, random_seed, num_folds):\n",
    "\n",
    "    the_c = 'gini'\n",
    "    if criterion == 1:\n",
    "        the_c = 'entropy'\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    training_features['Wilderness_Area'] = np.asarray(training_features['Wilderness_Area1']) + 2*np.asarray(training_features['Wilderness_Area2']) + 3*np.asarray(training_features['Wilderness_Area3']) + 4*np.asarray(training_features['Wilderness_Area4'])\n",
    "    training_features['Soil_Group'] = np.asarray(training_features['Soil_Group1']) + 2*np.asarray(training_features['Soil_Group2']) + 3*np.asarray(training_features['Soil_Group3']) + 4*np.asarray(training_features['Soil_Group4'])\n",
    "\n",
    "    training_features = training_features.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a decision tree classifier model for our data using Sklearn\n",
    "    decision_tree_model = DecisionTreeClassifier(criterion=the_c, max_depth=max_depth, random_state=random_seed)\n",
    "    \n",
    "    cross_validator_results = cross_validate(decision_tree_model, forest_data_scaled, forest_targets, scoring=['accuracy','f1_weighted'], cv=num_folds)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "            output_col: gr.Column(visible=True),\n",
    "            avg_accuracy: np.mean(cross_validator_results['test_accuracy']),\n",
    "            avg_stdev: np.std(cross_validator_results['test_accuracy'], ddof=1),\n",
    "            avg_f1: np.mean(cross_validator_results['test_f1_weighted']),\n",
    "            f1_stdev: np.std(cross_validator_results['test_f1_weighted'], ddof=1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e60d9e-6e5e-4ed1-a104-0bcac543dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rf(num_estimators, criterion, max_depth, random_seed, num_folds):\n",
    "\n",
    "    the_c = 'gini'\n",
    "    if criterion == 1:\n",
    "        the_c = 'entropy'\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    training_features, targets = forest_data_preprocessor(df)\n",
    "\n",
    "    training_features['Wilderness_Area'] = np.asarray(training_features['Wilderness_Area1']) + 2*np.asarray(training_features['Wilderness_Area2']) + 3*np.asarray(training_features['Wilderness_Area3']) + 4*np.asarray(training_features['Wilderness_Area4'])\n",
    "    training_features['Soil_Group'] = np.asarray(training_features['Soil_Group1']) + 2*np.asarray(training_features['Soil_Group2']) + 3*np.asarray(training_features['Soil_Group3']) + 4*np.asarray(training_features['Soil_Group4'])\n",
    "\n",
    "    training_features = training_features.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Soil_Group1','Soil_Group2','Soil_Group3','Soil_Group4'], axis=1)\n",
    "    \n",
    "    forest_data = np.asarray(training_features)\n",
    "    forest_targets = np.asarray(targets)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    forest_data_scaled = scaler.fit_transform(forest_data)\n",
    "\n",
    "\n",
    "    # Create a random forest classifier model for our data using Sklearn\n",
    "    rf_model = RandomForestClassifier(num_estimators, criterion=the_c, max_depth=max_depth, random_state=random_seed)\n",
    "    \n",
    "    cross_validator_results = cross_validate(rf_model, forest_data_scaled, forest_targets, scoring=['accuracy','f1_weighted'], cv=num_folds)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "            output_col: gr.Column(visible=True),\n",
    "            avg_accuracy: np.mean(cross_validator_results['test_accuracy']),\n",
    "            avg_stdev: np.std(cross_validator_results['test_accuracy'], ddof=1),\n",
    "            avg_f1: np.mean(cross_validator_results['test_f1_weighted']),\n",
    "            f1_stdev: np.std(cross_validator_results['test_f1_weighted'], ddof=1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c393dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column() as predictor:\n",
    "        elevation = gr.Number(100,label=\"Elevation\")\n",
    "        aspect = gr.Number(0,label=\"Aspect\")\n",
    "        slope = gr.Number(0,label=\"Slope\")\n",
    "        h_dist_hydro = gr.Number(100,label=\"Horizontal Distance To Hydrology\")\n",
    "        v_dist_hydro = gr.Number(100,label=\"Vertical Distance To Hydrology\")\n",
    "        h_dist_road = gr.Number(100,label=\"Horizontal Distance To Roadways\")\n",
    "        shade_9 = gr.Number(128,label=\"Hillshade 9 AM (0-255)\")\n",
    "        shade_12 = gr.Number(128,label=\"Hillshade Noon (0-255)\")\n",
    "        shade_3 = gr.Number(128,label=\"Hillshade 3 PM (0-255)\")\n",
    "        h_dist_fire = gr.Number(100,label=\"Horizontal Distance To Fire Points\")\n",
    "        wilderness = gr.Number(1,label=\"Wilderness Area (1-4)\")\n",
    "        soil = gr.Number(1,label=\"Soil Type (1-40)\")\n",
    "\n",
    "        with gr.Row() as predictor_row:\n",
    "            \n",
    "            with gr.Column() as log_reg_p:    \n",
    "                maximum_iter_p = gr.Number(600, label=\"Maximum Iterations\")\n",
    "                seed_val_p = gr.Number(13124, label=\"Random Seed Value\")\n",
    "                c_param_p = gr.Number(64, label=\"C\")\n",
    "                log_reg_predictor = gr.Button(\"Predict with Logistic Regression\")\n",
    "            with gr.Column() as knn_p:\n",
    "                k_neigh_p = gr.Number(1, label=\"Number of Neighbors\")\n",
    "                cat_scale_p = gr.Number(5, label=\"Scale for Categorical Attributes\")\n",
    "                knn_predictor = gr.Button(\"Predict with k-NN Classifier\")\n",
    "            with gr.Column() as d_tree_p:\n",
    "                criterion_p = gr.Number(1, label=\"Criterion (0: GINI, 1: Entropy)\")\n",
    "                seed_val2_p = gr.Number(13124, label=\"Random Seed Value\")\n",
    "                max_depth_p = gr.Number(13, label=\"Maximum Depth\")\n",
    "                d_tree_predictor = gr.Button(\"Predict with Decision Tree Classifier\")\n",
    "            with gr.Column() as rf_p:\n",
    "                num_est_p = gr.Number(150, label=\"Number of Estimators\")\n",
    "                criterion2_p = gr.Number(1, label=\"Criterion (0: GINI, 1: Entropy)\")\n",
    "                seed_val3_p = gr.Number(13124, label=\"Random Seed Value\")\n",
    "                max_depth2_p = gr.Number(19, label=\"Maximum Depth\")\n",
    "                rf_predictor = gr.Button(\"Predict with Random Forest Classifier\")\n",
    "\n",
    "        with gr.Column() as predictor_output:\n",
    "            forest_cover = gr.Text(label=\"Forest Cover Type\")\n",
    "            forest_cover_name = gr.Text(label=\"Forest Cover Type Name\")\n",
    "            \n",
    "        \n",
    "    with gr.Column() as misc_col:\n",
    "    \n",
    "        visualizer = gr.Button(\"Perform Data Visualization\")\n",
    "        bar_charts = gr.Plot(container=True, visible=False)\n",
    "        histograms = gr.Plot(container=True, visible=False)\n",
    "        \n",
    "        visualizer.click(fn=perform_visualization, outputs=[bar_charts,histograms])\n",
    "    \n",
    "        with gr.Row() as model_row:\n",
    "           \n",
    "            with gr.Column() as logRegCol:\n",
    "                maximum_iter = gr.Number(600, label=\"Maximum Iterations\")\n",
    "                seed_val = gr.Number(13124, label=\"Random Seed Value\")\n",
    "                c_param = gr.Number(64, label=\"C\")\n",
    "                tester1 = gr.Button(\"Cross Validate Logistic Regression\")\n",
    "    \n",
    "            with gr.Column() as knnCol:\n",
    "                k_neigh = gr.Number(1, label=\"Number of Neighbors\")\n",
    "                cat_scale = gr.Number(5, label=\"Scale for Categorical Attributes\")\n",
    "                tester2 = gr.Button(\"Cross Validate k-NN Classifier\")\n",
    "    \n",
    "            with gr.Column() as dTreeCol:\n",
    "                criterion = gr.Number(1, label=\"Criterion (0: GINI, 1: Entropy)\")\n",
    "                seed_val2 = gr.Number(13124, label=\"Random Seed Value\")\n",
    "                max_depth = gr.Number(13, label=\"Maximum Depth\")\n",
    "                tester3 = gr.Button(\"Cross Validate Decision Tree Classifier\")\n",
    "    \n",
    "            with gr.Column() as rfCol:\n",
    "                num_est = gr.Number(150, label=\"Number of Estimators\")\n",
    "                criterion2 = gr.Number(1, label=\"Criterion (0: GINI, 1: Entropy)\")\n",
    "                seed_val3 = gr.Number(13124, label=\"Random Seed Value\")\n",
    "                max_depth2 = gr.Number(19, label=\"Maximum Depth\")\n",
    "                tester4 = gr.Button(\"Cross Validate Random Forest Classifier\")\n",
    "               \n",
    "        cv_num_folds = gr.Number(7, label='Number of Folds for Cross Validation')\n",
    "        \n",
    "    \n",
    "        with gr.Column() as output_col:\n",
    "            avg_accuracy = gr.Number(label=\"Average Accuracy Score\")\n",
    "            avg_stdev = gr.Number(label=\"Accuracy Score Standard Deviation\")\n",
    "            avg_f1 = gr.Number(label=\"Average F1 Score\")\n",
    "            f1_stdev = gr.Number(label=\"F1 Score Standard Deviation\")\n",
    "\n",
    "    tester1.click(fn=do_logreg, inputs=[c_param,maximum_iter,seed_val,cv_num_folds], outputs=[output_col,avg_accuracy,avg_stdev,avg_f1,f1_stdev])\n",
    "    tester2.click(fn=do_knn, inputs=[k_neigh,cat_scale,cv_num_folds], outputs=[output_col,avg_accuracy,avg_stdev,avg_f1,f1_stdev])\n",
    "    tester3.click(fn=do_dtree, inputs=[criterion,max_depth,seed_val2,cv_num_folds], outputs=[output_col,avg_accuracy,avg_stdev,avg_f1,f1_stdev])\n",
    "    tester4.click(fn=do_rf, inputs=[num_est,criterion2,max_depth2,seed_val3,cv_num_folds], outputs=[output_col,avg_accuracy,avg_stdev,avg_f1,f1_stdev])\n",
    "\n",
    "    knn_predictor.click(fn=knn_predict, inputs=[elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,k_neigh_p,cat_scale_p], outputs=[forest_cover,forest_cover_name])\n",
    "    log_reg_predictor.click(fn=log_reg_predict, inputs=[elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,c_param_p,maximum_iter_p,seed_val_p], outputs=[forest_cover,forest_cover_name])\n",
    "    d_tree_predictor.click(fn=d_tree_predict, inputs=[elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,criterion_p,max_depth_p,seed_val2_p], outputs=[forest_cover,forest_cover_name])\n",
    "    rf_predictor.click(fn=rf_predict, inputs=[elevation,aspect,slope,h_dist_hydro,v_dist_hydro,h_dist_road,shade_9,shade_12,shade_3,h_dist_fire,wilderness,soil,num_est_p,criterion2_p,max_depth2_p,seed_val3_p], outputs=[forest_cover,forest_cover_name])\n",
    "\n",
    "    \n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
